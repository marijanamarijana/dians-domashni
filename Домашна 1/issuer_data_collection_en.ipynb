{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTj6QYiqpwyLjRWTe2Jq/T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Направивме споредба помеѓу двете верзии на податоците (англиската и\n","македонската) и воочивме дека нема никаква разлика помеѓу собраните податоци."],"metadata":{"id":"xhG90XuOH1il"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhNs218OHkbs"},"outputs":[],"source":["import aiohttp\n","import asyncio\n","import pandas as pd\n","from io import BytesIO\n","from bs4 import BeautifulSoup\n","import re\n","from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor\n","import random"]},{"cell_type":"code","source":["class Pipeline(object):\n","  def __init__(self):\n","    self.filters = []\n","\n","  def connect(self, filter):\n","    self.filters.append(filter)\n","    return self\n","\n","  async def combine_data(self, session, url, data):\n","    return await self.filters[-1].process(session, url, data)\n","\n","  async def process_data(self, session, url, data):\n","    for filter in self.filters[1:-1]:\n","      data = await filter.process(session, url, data)\n","    return data\n","\n","  async def execute(self, url):\n","      final_data = []\n","      async with aiohttp.ClientSession() as session:\n","        start_data = await self.filters[0].process(session, url, [])\n","\n","        processing_tasks = [self.process_data(session, url, data) for data in start_data]\n","        results = await asyncio.gather(*processing_tasks)\n","\n","        final_data = await self.combine_data(session, url, results)\n","\n","      return final_data"],"metadata":{"id":"XLBmxXzZH5eV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Filter(object):\n","  async def process(self, session, url, data):\n","    raise NotImplementedError()"],"metadata":{"id":"dkqc--UhH6t9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ValidIssuersFilter(Filter):\n","\n","  async def get_issuers(self, session, url):\n","    async with session.get(url) as response:\n","      response.raise_for_status()\n","      data = await response.text()\n","      soup = BeautifulSoup(data, \"lxml\")\n","      options = soup.select(\"#Code > option\")\n","      return [option.text.strip() for option in options if option and option.text.strip()]\n","\n","  async def process(self, session, url, data):\n","    issuers = await self.get_issuers(session, url)\n","    pattern = re.compile(r\"^(?!M.*\\d)[^\\d]*$\")\n","    return [issuer for issuer in issuers if pattern.search(issuer)]\n"],"metadata":{"id":"_K-OJjkyH7v1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IssuerDatesFilter(Filter):\n","  def __init__(self):\n","    self.prev_data = load_data()\n","\n","  def get_latest_date(self, df, issuer):\n","    issuer_data = df[df['Issuer'] == issuer]\n","    dates = pd.to_datetime(issuer_data['Date'], format='%m/%d/%Y', errors='coerce')\n","    return dates.max() if not issuer_data.empty else (pd.Timestamp.now() - pd.DateOffset(years=10)).normalize()\n","\n","  def generate_date_pairs(self, latest_date):\n","    now = pd.Timestamp.today().normalize()\n","    pairs = []\n","\n","    start_date = latest_date + pd.Timedelta(days=1)\n","\n","    while start_date <= now:\n","      end_date = min(start_date + pd.Timedelta(days=364), now)\n","      pairs.append((start_date, end_date))\n","      start_date = end_date + pd.Timedelta(days=1)\n","    return pairs\n","\n","  async def process(self, session, url, data):\n","    issuer_date_pairs = {}\n","    ten_years_ago = (pd.Timestamp.now() - pd.DateOffset(years=10)).normalize()\n","    if self.prev_data.empty or data not in self.prev_data['Issuer'].unique():\n","      latest_date = ten_years_ago\n","    else:\n","      latest_date = self.get_latest_date(self.prev_data, data)\n","\n","    date_pairs = self.generate_date_pairs(latest_date)\n","    issuer_date_pairs[data] = date_pairs\n","    return issuer_date_pairs"],"metadata":{"id":"f11FPVcfH9F1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FillInIssuerDataFilter(Filter):\n","\n","  def __init__(self):\n","    self.semaphore = asyncio.Semaphore(30)\n","\n","  def parse_row(self, row, issuer):\n","    data = [ele.text.strip() for ele in row.select('td')]\n","    if data:\n","      data.insert(0, issuer)\n","    return data\n","\n","  def parse_table(self, soup, issuer):\n","    rows = soup.select(\"tbody tr\")\n","    return [self.parse_row(row, issuer) for row in rows if row.select(\"td\")]\n","\n","\n","  async def fetch_data(self, session, url, data):\n","    timeout = aiohttp.ClientTimeout(total=20)\n","    retries = 8\n","\n","    for attempt in range(retries):\n","      try:\n","        async with session.post(url, data=data, timeout=timeout) as response:\n","          if response.status == 200:\n","            return await response.text()\n","          else:\n","            return None\n","      except asyncio.TimeoutError:\n","        pass\n","      except aiohttp.ClientError as e:\n","        pass\n","      await asyncio.sleep(2 ** attempt + random.uniform(0, 1))\n","    return None\n","\n","\n","  async def download_data(self, session, issuer, from_date, to_date):\n","    url = 'https://www.mse.mk/en/stats/symbolhistory/' + issuer\n","    payload = {\n","      'Code': issuer,\n","      'FromDate': from_date,\n","      'ToDate': to_date\n","    }\n","    async with self.semaphore:\n","      response = await self.fetch_data(session, url, payload)\n","      if response is None:\n","        return None\n","\n","    return response\n","\n","\n","  async def download_latest_issuer_data(self, session, url, issuer, date_pairs):\n","    tasks = [self.download_data(session, issuer, from_date, to_date) for from_date, to_date in date_pairs]\n","\n","    results = await asyncio.gather(*tasks, return_exceptions=True)\n","    valid_results = [result for result in results if result is not None]\n","\n","    collected_rows = []\n","    loop = asyncio.get_event_loop()\n","\n","    with ThreadPoolExecutor() as executor:\n","      parsing_tasks = [\n","        loop.run_in_executor(executor, self.parse_table, BeautifulSoup(result, \"lxml\"), issuer)\n","        for result in valid_results if result\n","      ]\n","      parsed_tables = await asyncio.gather(*parsing_tasks)\n","\n","    for data in parsed_tables:\n","      if data:\n","        collected_rows.extend(data)\n","\n","    return collected_rows\n","\n","\n","  async def process(self, session, url, data):\n","    issuer = list(data.keys())[0]\n","    res = await self.download_latest_issuer_data(session, url, issuer, data[issuer])\n","    return res"],"metadata":{"id":"82N_x1c7H-bC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CombineAndSaveFilter(Filter):\n","  def __init__(self, file_path):\n","    self.file_path = file_path\n","\n","  def parse_col_names(self, soup):\n","    col_names = [\"Issuer\"] + [th.text.strip() for th in soup.select(\"th\") if th.text]\n","    return col_names\n","\n","  async def get_col_names(self, session, url):\n","    async with session.get(url) as response:\n","      response.raise_for_status()\n","      data = await response.text()\n","      soup = BeautifulSoup(data, \"lxml\")\n","      return self.parse_col_names(soup)\n","\n","\n","  async def process(self, session, url, data):\n","    col_names = await self.get_col_names(session, url)\n","\n","    prev_data = load_data(self.file_path)\n","\n","    data = [d for d in data if d is not None]\n","    data = [sublist for group in data for sublist in group]\n","\n","    new_data = pd.DataFrame(data, columns=col_names) if data else pd.DataFrame(columns=col_names)\n","    new_data = new_data.dropna(axis=1, how='all')\n","\n","    combined_data = pd.concat([prev_data, new_data], ignore_index=True) \\\n","      .drop_duplicates(subset=[\"Issuer\", \"Date\"], keep=\"last\")\n","\n","    combined_data[\"Date\"] = pd.to_datetime(combined_data[\"Date\"], format='%m/%d/%Y').dt.strftime('%d.%m.%Y')\n","\n","    combined_data.iloc[:, 2:] = combined_data.iloc[:, 2:].apply(lambda x:\n","    x.where(x.isna(), x.astype(str))\n","     .str.replace(r'\\.', '_', regex=True) \\\n","     .str.replace(',', '.', regex=True) \\\n","     .str.replace('_', ',', regex=True) \\\n","    )\n","\n","    combined_data.to_csv(\"issuers_data_en.csv\", index=False, chunksize=5000)\n"],"metadata":{"id":"5GFRobexH_C1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(file_path=\"issuers_data_en.csv\"):\n","  try:\n","    data = pd.read_csv(file_path, chunksize=5000)\n","    issuers_data = pd.concat(data, ignore_index=True)\n","  except:\n","    issuers_data = pd.DataFrame()\n","  return issuers_data"],"metadata":{"id":"fWN4MO7fIAzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def main():\n","  file_path = \"issuers_data_en.csv\"\n","  url = \"https://www.mse.mk/en/stats/symbolhistory/ADIN\"\n","\n","  pipeline = Pipeline()\n","  pipeline.connect(ValidIssuersFilter()) \\\n","    .connect(IssuerDatesFilter()) \\\n","    .connect(FillInIssuerDataFilter()) \\\n","    .connect(CombineAndSaveFilter(file_path=file_path))\n","\n","  start_time = datetime.now()\n","  await pipeline.execute(url)\n","  end_time = datetime.now()\n","\n","  elapsed_time = end_time - start_time\n","  print(f\"Total execution time: {elapsed_time}\")"],"metadata":{"id":"0tYCUO3PICDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["await main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKyV16q9IEeW","executionInfo":{"status":"ok","timestamp":1731025384093,"user_tz":-60,"elapsed":438889,"user":{"displayName":"Natalija Naseva","userId":"00698395416279922217"}},"outputId":"7bc54d4b-d827-496e-ffdc-2eaac6a5b372"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 0:07:18.789413\n"]}]}]}