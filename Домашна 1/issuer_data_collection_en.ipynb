{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Направивме споредба помеѓу двете верзии на податоците (англиската и\n",
        "македонската) и воочивме дека нема никаква разлика помеѓу собраните податоци."
      ],
      "metadata": {
        "id": "xhG90XuOH1il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp\n",
        "!pip install pandas\n",
        "!pip install lxml"
      ],
      "metadata": {
        "id": "0SkUvzITszzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vhNs218OHkbs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from lxml import etree\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline(object):\n",
        "  def __init__(self):\n",
        "    self.filters = []\n",
        "\n",
        "  def connect(self, filter):\n",
        "    self.filters.append(filter)\n",
        "    return self\n",
        "\n",
        "  async def combine_data(self, session, url, data):\n",
        "    return await self.filters[-1].process(session, url, data)\n",
        "\n",
        "  async def process_data(self, session, url, data):\n",
        "    for filter in self.filters[1:-1]:\n",
        "      data = await filter.process(session, url, data)\n",
        "    return data\n",
        "\n",
        "  async def execute(self, url, issuer_urls):\n",
        "      final_data = []\n",
        "      async with aiohttp.ClientSession() as session:\n",
        "        start_data = await self.filters[0].process(session, issuer_urls, [])\n",
        "        processing_tasks = [self.process_data(session, url, data) for data in start_data]\n",
        "        results = await asyncio.gather(*processing_tasks)\n",
        "\n",
        "        final_data = await self.combine_data(session, url, results)\n",
        "\n",
        "      return final_data"
      ],
      "metadata": {
        "id": "XLBmxXzZH5eV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filter(object):\n",
        "  async def process(self, session, url, data):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "dkqc--UhH6t9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidIssuersFilter(Filter):\n",
        "\n",
        "  async def get_issuers(self, session, urls):\n",
        "    issuers = []\n",
        "    async def fetch_issuers(url):\n",
        "      async with session.get(url) as response:\n",
        "        response.raise_for_status()\n",
        "        data = await response.text()\n",
        "        tree = etree.HTML(data)\n",
        "        options = tree.xpath(\"//tbody//tr//td//a\")\n",
        "        return [option.text.strip() for option in options if option.text.strip()]\n",
        "\n",
        "    results = await asyncio.gather(*[fetch_issuers(url) for url in urls])\n",
        "\n",
        "    for result in results:\n",
        "      issuers.extend(result)\n",
        "    return issuers\n",
        "\n",
        "\n",
        "  async def process(self, session, url, data):\n",
        "    issuers = await self.get_issuers(session, url)\n",
        "    pattern = re.compile(r\"^[^\\d]*$\")\n",
        "    return list(set([issuer for issuer in issuers if pattern.search(issuer)]))\n"
      ],
      "metadata": {
        "id": "_K-OJjkyH7v1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IssuerDatesFilter(Filter):\n",
        "  def __init__(self):\n",
        "    self.prev_data = load_data()\n",
        "\n",
        "  def get_latest_date(self, df, issuer):\n",
        "    issuer_data = df[df['Issuer'] == issuer]\n",
        "    dates = pd.to_datetime(issuer_data['Date'], format='%m/%d/%Y', errors='coerce')\n",
        "    return dates.max() if not issuer_data.empty else (pd.Timestamp.now() - pd.DateOffset(years=10)).normalize()\n",
        "\n",
        "  def generate_date_pairs(self, latest_date):\n",
        "    now = pd.Timestamp.today().normalize()\n",
        "    pairs = []\n",
        "\n",
        "    start_date = latest_date + pd.Timedelta(days=1)\n",
        "\n",
        "    while start_date <= now:\n",
        "      end_date = min(start_date + pd.Timedelta(days=364), now)\n",
        "      pairs.append((start_date, end_date))\n",
        "      start_date = end_date + pd.Timedelta(days=1)\n",
        "    return pairs\n",
        "\n",
        "  async def process(self, session, url, data):\n",
        "    issuer_date_pairs = {}\n",
        "    ten_years_ago = (pd.Timestamp.now() - pd.DateOffset(years=10)).normalize()\n",
        "    if self.prev_data.empty or data not in self.prev_data['Issuer'].unique():\n",
        "      latest_date = ten_years_ago\n",
        "    else:\n",
        "      latest_date = self.get_latest_date(self.prev_data, data)\n",
        "\n",
        "    date_pairs = self.generate_date_pairs(latest_date)\n",
        "    issuer_date_pairs[data] = date_pairs\n",
        "    return issuer_date_pairs"
      ],
      "metadata": {
        "id": "f11FPVcfH9F1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FillInIssuerDataFilter(Filter):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.semaphore = asyncio.Semaphore(30)\n",
        "\n",
        "  def parse_row(self, row, issuer):\n",
        "    data = [ele.strip() for ele in row]\n",
        "    if data:\n",
        "      data.insert(0, issuer)\n",
        "    return data\n",
        "\n",
        "  def parse_table(self, html_content, issuer):\n",
        "    tree = etree.HTML(html_content)\n",
        "    rows = tree.xpath(\"//tbody//tr\")\n",
        "    return [\n",
        "        self.parse_row([ele.text.strip() if ele.text else '' for ele in row.xpath(\".//td\")], issuer)\n",
        "        for row in rows\n",
        "    ]\n",
        "\n",
        "\n",
        "  async def fetch_data(self, session, url, data):\n",
        "    timeout = aiohttp.ClientTimeout(total=20)\n",
        "    retries = 8\n",
        "\n",
        "    for attempt in range(retries):\n",
        "      try:\n",
        "        async with session.post(url, data=data, timeout=timeout) as response:\n",
        "          if response.status == 200:\n",
        "            return await response.text()\n",
        "          else:\n",
        "            return None\n",
        "      except asyncio.TimeoutError:\n",
        "        pass\n",
        "      except aiohttp.ClientError as e:\n",
        "        pass\n",
        "      await asyncio.sleep(2 ** attempt + random.uniform(0, 1))\n",
        "    return None\n",
        "\n",
        "\n",
        "  async def download_data(self, session, issuer, from_date, to_date):\n",
        "    url = 'https://www.mse.mk/en/stats/symbolhistory/' + issuer\n",
        "    payload = {\n",
        "      'Code': issuer,\n",
        "      'FromDate': from_date,\n",
        "      'ToDate': to_date\n",
        "    }\n",
        "    async with self.semaphore:\n",
        "      response = await self.fetch_data(session, url, payload)\n",
        "      if response is None:\n",
        "        return None\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "  async def download_latest_issuer_data(self, session, url, issuer, date_pairs):\n",
        "    tasks = [self.download_data(session, issuer, from_date, to_date) for from_date, to_date in date_pairs]\n",
        "\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    valid_results = [result for result in results if result is not None]\n",
        "\n",
        "    collected_rows = []\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "      parsing_tasks = [\n",
        "        loop.run_in_executor(executor, self.parse_table, result, issuer)\n",
        "        for result in valid_results if result\n",
        "      ]\n",
        "      parsed_tables = await asyncio.gather(*parsing_tasks)\n",
        "\n",
        "    for data in parsed_tables:\n",
        "      if data:\n",
        "        collected_rows.extend(data)\n",
        "\n",
        "    return collected_rows\n",
        "\n",
        "\n",
        "  async def process(self, session, url, data):\n",
        "    issuer = list(data.keys())[0]\n",
        "    res = await self.download_latest_issuer_data(session, url, issuer, data[issuer])\n",
        "    return res"
      ],
      "metadata": {
        "id": "82N_x1c7H-bC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombineAndSaveFilter(Filter):\n",
        "  def __init__(self, file_path):\n",
        "    self.file_path = file_path\n",
        "\n",
        "  def parse_col_names(self, tree):\n",
        "    col_names = [\"Issuer\"] + [th.text.strip() for th in tree.xpath(\"//th\") if th.text]\n",
        "    return col_names\n",
        "\n",
        "  async def get_col_names(self, session, url):\n",
        "    async with session.get(url) as response:\n",
        "      response.raise_for_status()\n",
        "      data = await response.text()\n",
        "      parser = etree.HTMLParser()\n",
        "      tree = etree.fromstring(data, parser)\n",
        "      return self.parse_col_names(tree)\n",
        "\n",
        "\n",
        "  async def process(self, session, url, data):\n",
        "    col_names = await self.get_col_names(session, url)\n",
        "\n",
        "    prev_data = load_data(self.file_path)\n",
        "\n",
        "    data = [d for d in data if d is not None]\n",
        "    data = [sublist for group in data for sublist in group]\n",
        "\n",
        "    new_data = pd.DataFrame(data, columns=col_names) if data else pd.DataFrame(columns=col_names)\n",
        "    new_data = new_data.dropna(axis=1, how='all')\n",
        "\n",
        "    combined_data = pd.concat([prev_data, new_data], ignore_index=True) \\\n",
        "      .drop_duplicates(subset=[\"Issuer\", \"Date\"], keep=\"last\")\n",
        "\n",
        "    combined_data.iloc[:, 2:] = combined_data.iloc[:, 2:].apply(lambda x:\n",
        "      x.where(x.isna(), x.astype(str))\n",
        "      .str.replace(r'\\.', '_', regex=True) \\\n",
        "      .str.replace(',', '.', regex=True) \\\n",
        "      .str.replace('_', ',', regex=True) \\\n",
        "    )\n",
        "\n",
        "    combined_data.to_csv(\"issuers_data_en.csv\", index=False, chunksize=5000)\n"
      ],
      "metadata": {
        "id": "5GFRobexH_C1"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path=\"issuers_data_en.csv\"):\n",
        "  try:\n",
        "    data = pd.read_csv(file_path, chunksize=5000)\n",
        "    issuers_data = pd.concat(data, ignore_index=True)\n",
        "  except:\n",
        "    issuers_data = pd.DataFrame()\n",
        "  return issuers_data"
      ],
      "metadata": {
        "id": "fWN4MO7fIAzG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "  file_path = \"issuers_data_en.csv\"\n",
        "  url = \"https://www.mse.mk/en/stats/symbolhistory/ADIN\"\n",
        "  issuer_urls = [\"https://www.mse.mk/en/stats/current-schedule#results-continuousTradingMode\", \"https://www.mse.mk/en/stats/current-schedule#results-fixingWith20PercentLimit\", \"https://www.mse.mk/en/stats/current-schedule#results-fixingWithoutLimit\"]\n",
        "\n",
        "  pipeline = Pipeline()\n",
        "  pipeline.connect(ValidIssuersFilter()) \\\n",
        "    .connect(IssuerDatesFilter()) \\\n",
        "    .connect(FillInIssuerDataFilter()) \\\n",
        "    .connect(CombineAndSaveFilter(file_path=file_path))\n",
        "\n",
        "  start_time = datetime.now()\n",
        "  await pipeline.execute(url, issuer_urls)\n",
        "  end_time = datetime.now()\n",
        "\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Total execution time: {elapsed_time}\")"
      ],
      "metadata": {
        "id": "0tYCUO3PICDt"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "lKyV16q9IEeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}