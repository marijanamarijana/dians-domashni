{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4858a-38b7-45eb-b231-c9c9243eec5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install aiohttp\n",
    "!pip install pandas\n",
    "!pip install psycopg2\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install PyMuPDF\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422cbf8-4791-48a1-8738-62df01eeeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import fitz\n",
    "import torch\n",
    "import aiohttp\n",
    "from aiohttp import ClientTimeout\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from transformers import pipeline\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa27efee-a140-47d3-b33c-549c16d8eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseHelper:\n",
    "    def __init__(self, connection_string):\n",
    "        self.connection_string = connection_string\n",
    "        self.engine = create_engine(self.connection_string)\n",
    "\n",
    "    def get_issuers(self):\n",
    "        query = \"SELECT issuer_name FROM issuers\"\n",
    "        with self.engine.connect() as conn:\n",
    "            result = conn.execute(text(query)).fetchall()\n",
    "        return [r[0] for r in result]\n",
    "    \n",
    "    def get_latest_date(self, issuer):\n",
    "        query = \"SELECT MAX(scraped_date) FROM news_sentiment WHERE issuer_name = :issuer\"\n",
    "        with self.engine.connect() as conn:\n",
    "            result = conn.execute(text(query), {'issuer': issuer}).fetchone()\n",
    "        return result[0] if result else None\n",
    "\n",
    "    def save_data(self, data):\n",
    "        query = \"\"\"\n",
    "            INSERT INTO news_sentiment (issuer_name, recommendation, scraped_date)\n",
    "            VALUES (:issuer_name, :recommendation, :scraped_date)\n",
    "        \"\"\"\n",
    "    \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(query), [\n",
    "                {\"issuer_name\": issuer, \"recommendation\": recommendation, \"scraped_date\": scraped_date}\n",
    "                for issuer, recommendation, scraped_date in data\n",
    "            ])\n",
    "            conn.commit() \n",
    "\n",
    "\n",
    "\n",
    "    def update_current_recommendations(self):\n",
    "        query = \"\"\"\n",
    "            WITH RecommendationCounts AS (\n",
    "                SELECT \n",
    "                    issuer_name, \n",
    "                    recommendation, \n",
    "                    COUNT(*) AS count,\n",
    "                    RANK() OVER (PARTITION BY issuer_name ORDER BY COUNT(*) DESC) AS rank\n",
    "                FROM news_sentiment\n",
    "                WHERE scraped_date >= CURRENT_DATE - INTERVAL '60 days'\n",
    "                GROUP BY issuer_name, recommendation\n",
    "            )\n",
    "            UPDATE issuers\n",
    "            SET current_recommendation = rc.recommendation\n",
    "            FROM RecommendationCounts rc\n",
    "            WHERE issuers.issuer_name = rc.issuer_name AND rc.rank = 1;\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527c373-9ac9-4639-88ee-07cbbfac487f",
   "metadata": {},
   "source": [
    "# SELENIUM SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2d1509-25ef-4f22-b7da-c7668896d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverManager:\n",
    "\n",
    "    def __init__(self, download_dir, parser):\n",
    "        self.download_dir = download_dir\n",
    "        self.driver = self.setup_driver()\n",
    "        self.parser = parser\n",
    "\n",
    "    def setup_driver(self):\n",
    "        if not os.path.exists(self.download_dir):\n",
    "            os.makedirs(self.download_dir)\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\") \n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        prefs = {\n",
    "            \"download.default_directory\": os.path.abspath(self.download_dir),\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        return driver\n",
    "\n",
    "    def wait_for_download(self, timeout=120):\n",
    "        initial_files = set(os.listdir(os.path.abspath(self.download_dir)))\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "            current_files = set(os.listdir(os.path.abspath(self.download_dir)))\n",
    "            new_files = current_files - initial_files\n",
    "            if new_files: \n",
    "                print(f\"New file downloaded: {new_files}\")\n",
    "                return new_files\n",
    "            if time.time() - start_time > timeout:\n",
    "                return None\n",
    "    \n",
    "    def fetch_article(self, url):\n",
    "        self.driver.get(url)\n",
    "        try:\n",
    "            data = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@class=\"container\"]/div[@class=\"row\"][4]//p[2]'))\n",
    "            )\n",
    "            return self.parser.clean_text(data.text)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            data = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@class=\"container\"]/div[@class=\"row\"][6]/div/strong/div'))\n",
    "            )\n",
    "            file_name = data.text\n",
    "            if \".pdf\" in file_name:\n",
    "                data.click()\n",
    "                count = self.wait_for_download()\n",
    "                if count:\n",
    "                    return self.parser.parse_file(file_name)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93064d9d-411f-4476-958c-b341c9ed332d",
   "metadata": {},
   "source": [
    "# ARTICLE PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edf8f9c-1a4d-4866-97c8-e4f6f2c2e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser():\n",
    "\n",
    "    def parse_file(self, file_name):\n",
    "        file_path = f\"news/{file_name}\"\n",
    "        file = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(min(2, file.page_count)): \n",
    "            page = file.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        text = self.clean_text(text)\n",
    "        print(text)\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = ' '.join(text.split())\n",
    "        text = re.sub(r'\\s+', ' ', text) \n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        print(text)\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93b281-647d-4125-a4f9-b933cd4222a0",
   "metadata": {},
   "source": [
    "# MSE ISSUER NEWS LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da17321-0916-4dd9-b434-f615d4736375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "        self.url = \"https://www.mse.mk/en/symbol/{issuer}\"\n",
    "\n",
    "    def extract_date(self, link_text):\n",
    "        date_pattern = r'(\\d{1,2}/\\d{1,2}/\\d{4})'\n",
    "        match = re.search(date_pattern, link_text)\n",
    "        if match:\n",
    "            date_string = match.group(1)  \n",
    "            date = datetime.strptime(date_string, '%m/%d/%Y')\n",
    "            date = date.strftime('%Y-%m-%d')\n",
    "            return date\n",
    "\n",
    "    #returns tuples of link and dates\n",
    "    async def get_issuer_news_links(self, issuer):\n",
    "        url = self.url.format(issuer=issuer)\n",
    "        async with self.session.get(url) as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.text()\n",
    "            tree = etree.HTML(data)\n",
    "            links = tree.xpath('//*[@id=\"seiNetIssuerLatestNews\"]//a')\n",
    "            links = [\n",
    "                (link.xpath('./@href')[0],\n",
    "                self.extract_date(link.xpath('./ul/li[2]/h4/text()')[0])) \n",
    "                for link in links\n",
    "                if link.xpath('./@href')\n",
    "            ]\n",
    "            return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d27364-a1c3-46bc-85d2-60713dc3a4b2",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d4cbab-694f-49c5-822d-723e6afeb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Analyzer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    def get_action(self, label):\n",
    "        actions = {\"positive\":\"buy\", \"negative\":\"sell\", \"neutral\":\"hold\"}\n",
    "        return actions[label]\n",
    "\n",
    "    def get_label(self, prediction):\n",
    "        labels = self.model.config.id2label\n",
    "        return labels[prediction.item()]\n",
    "\n",
    "    # returns recommendation,\n",
    "    def analyze_article(self, text):\n",
    "        inputs = self.tokenizer(text, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, axis=-1)\n",
    "        label = self.get_label(prediction)\n",
    "        return self.get_action(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc076945-6117-4652-b18e-7612175b91f4",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f68fc5b-5c32-4ca7-9310-85061f091f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def __init__(self, db_connection_string):\n",
    "        self.db = DatabaseHelper(db_connection_string)\n",
    "        self.parser = Parser()\n",
    "        self.driver_manager = DriverManager(\"news\", self.parser)\n",
    "        self.sentiment_analyzer = Sentiment_Analyzer()\n",
    "        \n",
    "\n",
    "    async def process_issuer(self, issuer, session):\n",
    "        latest_date = self.db.get_latest_date(issuer)\n",
    "        print(f\"{issuer}: {latest_date}\")\n",
    "        scraper = Scraper(session)\n",
    "        news_links = await scraper.get_issuer_news_links(issuer)\n",
    "        print(f\"LINKS -> {issuer}: {news_links}\")\n",
    "\n",
    "\n",
    "        filtered_links = [(link, date) for link, date in news_links if date > (latest_date or '1900-01-01')]\n",
    "\n",
    "        tasks = [\n",
    "            self.process_article(issuer, link, date)\n",
    "            for link, date in filtered_links\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        final_results = []\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                final_results.append(result)\n",
    "        \n",
    "        # Save to database\n",
    "        print(f\"DATA -> {issuer}: {results}\")\n",
    "        if results:\n",
    "            self.db.save_data(results)\n",
    "\n",
    "\n",
    "    async def process_article(self, issuer, link, date):\n",
    "        content = self.driver_manager.fetch_article(link)\n",
    "        if content:\n",
    "            recommendation = self.sentiment_analyzer.analyze_article(content)\n",
    "            return issuer, recommendation, datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "    async def process_issuer_with_retry(self, issuer, session, retries=3, timeout=60):\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                await asyncio.wait_for(self.process_issuer(issuer, session), timeout)\n",
    "                break \n",
    "            except asyncio.TimeoutError:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout exceeded while processing {issuer}. Attempt {attempt}/{retries}\")\n",
    "                if attempt == retries:\n",
    "                    print(f\"Max retries reached for {issuer}. Skipping this issuer.\")\n",
    "                    break\n",
    "                await asyncio.sleep(2**attempt)\n",
    "            except asyncio.CancelledError:\n",
    "                print(f\"Task for {issuer} was cancelled.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred with {issuer}: {e}\")\n",
    "                break\n",
    "\n",
    "    async def run(self):\n",
    "        issuers = self.db.get_issuers()\n",
    "        async with aiohttp.ClientSession(timeout=ClientTimeout(total=120)) as session:\n",
    "            tasks = [self.process_issuer_with_retry(issuer, session) for issuer in issuers]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac88810-05cd-4d29-b7f5-8213ad0b76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\"postgresql+psycopg2://sa:p123@localhost:5432/Makcii_DB\")\n",
    "await pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883458d-fe3f-4e0b-9025-f996e46bc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     pipeline = Pipeline(\"postgresql+psycopg2://sa:p123@postgres:5432/Makcii_DB\")\n",
    "#     try:\n",
    "#         asyncio.run(pipeline.run())\n",
    "#     finally:\n",
    "#         pipeline.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a8e78-7390-415c-93bd-a32338661d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
